# RAG Agent ğŸ¤–

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

Uma biblioteca Python para RAG (Retrieval-Augmented Generation) **estrita** que responde apenas com base no contexto recuperado dos documentos indexados.

## âœ¨ CaracterÃ­sticas

- **ğŸ¯ RAG Estrito**: Responde somente se a informaÃ§Ã£o estiver claramente no contexto dos documentos
- **ğŸ”Œ Provedores PlugÃ¡veis**: Suporte para embeddings e LLMs locais ou via API (OpenAI/Ollama)
- **ğŸ“„ MÃºltiplos Formatos**: Processa arquivos TXT, MD e PDF
- **ğŸ“Š Logging Estruturado**: Logs em formato JSON com rastreamento de requisiÃ§Ãµes
- **ğŸ“ Filtragem por DistÃ¢ncia**: Usa threshold de distÃ¢ncia cosseno para relevÃ¢ncia
- **ğŸ§ª Testado**: Suite completa de testes unitÃ¡rios e de integraÃ§Ã£o

## ğŸš€ InstalaÃ§Ã£o RÃ¡pida

```bash
# Clone o repositÃ³rio
git clone https://github.com/marcosf63/rag-agent.git
cd rag-agent

# Instale em modo desenvolvimento
pip install -e ".[all]"
```

## ğŸ“– Uso BÃ¡sico

```python
from rag_agent import (
    RagAgent, 
    ChromaStore, 
    SentenceTransformerEmbedding,
    OllamaChat,
    ingest_file,
    AnswerNotFoundError
)

# 1. Configurar embedding (local - sem API)
embedder = SentenceTransformerEmbedding()

# 2. Criar vector store
store = ChromaStore("meus_docs", embedder)

# 3. Indexar documentos
ingest_file("documento.pdf", store)

# 4. Configurar LLM (Ollama local)
llm = OllamaChat()

# 5. Criar agente
agent = RagAgent(store=store, llm=llm)

# 6. Fazer pergunta
try:
    resultado = agent.ask("Qual procedimento estÃ¡ descrito no documento?")
    print(f"âœ… {resultado['answer']}")
    print(f"ğŸ“š Chunks: {[c['chunk_id'] for c in resultado['used_chunks']]}")
except AnswerNotFoundError:
    print("âŒ InformaÃ§Ã£o nÃ£o encontrada nos documentos.")
```

## ğŸ› ï¸ ConfiguraÃ§Ã£o de Provedores

### Embeddings

**Local (sem API)**:
```python
from rag_agent import SentenceTransformerEmbedding
embedder = SentenceTransformerEmbedding("all-MiniLM-L6-v2")
```

**OpenAI** (requer `OPENAI_API_KEY`):
```python
from rag_agent import OpenAIEmbedding
embedder = OpenAIEmbedding("text-embedding-3-small")
```

### LLMs

**Ollama Local**:
```python
from rag_agent import OllamaChat
llm = OllamaChat("llama3.1:8b")  # Requer Ollama rodando
```

**OpenAI** (requer `OPENAI_API_KEY`):
```python
from rag_agent import OpenAIChat
llm = OpenAIChat("gpt-4o-mini")
```

## ğŸ“ Estrutura do Projeto

```
rag_agent/
â”œâ”€â”€ src/rag_agent/          # ğŸ“¦ CÃ³digo fonte da biblioteca
â”‚   â”œâ”€â”€ core/               # ğŸ§  Componentes principais
â”‚   â”œâ”€â”€ providers/          # ğŸ”Œ Provedores de embedding/LLM  
â”‚   â”œâ”€â”€ storage/            # ğŸ’¾ Armazenamento vetorial
â”‚   â””â”€â”€ utils/              # ğŸ› ï¸ UtilitÃ¡rios
â”œâ”€â”€ tests/                  # ğŸ§ª Testes (unitÃ¡rios + integraÃ§Ã£o)
â”œâ”€â”€ examples/               # ğŸ“š Exemplos de uso
â”œâ”€â”€ config/                 # âš™ï¸ ConfiguraÃ§Ãµes
â””â”€â”€ docs/                   # ğŸ“– DocumentaÃ§Ã£o
```

## ğŸ”§ Comandos Ãšteis

```bash
# Testes
make test              # Todos os testes
make test-unit         # Apenas testes unitÃ¡rios
make test-integration  # Apenas testes de integraÃ§Ã£o

# Qualidade do cÃ³digo
make lint              # Linting
make format            # FormataÃ§Ã£o automÃ¡tica
make type-check        # VerificaÃ§Ã£o de tipos

# Desenvolvimento
make install-dev       # Instalar com deps de desenvolvimento
make run-example       # Executar exemplo bÃ¡sico
```

## ğŸ“Š Estrutura de Resposta

```json
{
  "request_id": "uuid-da-requisiÃ§Ã£o",
  "answer": "Resposta baseada no contexto recuperado",
  "used_chunks": [
    {
      "chunk_id": 0,
      "distance": 0.15,
      "source": "documento.pdf"
    }
  ],
  "latency_ms": 1250.5
}
```

## âš™ï¸ ParÃ¢metros de ConfiguraÃ§Ã£o

| ParÃ¢metro | PadrÃ£o | DescriÃ§Ã£o |
|-----------|---------|-----------|
| `top_k` | 5 | NÃºmero de chunks recuperados |
| `max_context_chars` | 4000 | Tamanho mÃ¡ximo do contexto |
| `distance_threshold` | 0.35 | Threshold de distÃ¢ncia cosseno |
| `max_chars` | 1200 | Tamanho dos chunks |
| `overlap` | 120 | SobreposiÃ§Ã£o entre chunks |

## ğŸš¨ Tratamento de Erros

- `AnswerNotFoundError`: InformaÃ§Ã£o nÃ£o encontrada nos documentos
- `RetrievalError`: Falha na busca vetorial  
- `LLMError`: Falha na geraÃ§Ã£o de resposta
- `EmbeddingError`: Falha na geraÃ§Ã£o de embeddings
- `IngestionError`: Falha no processamento de documentos

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/nova-feature`)
3. Commit suas mudanÃ§as
4. Push para a branch (`git push origin feature/nova-feature`)
5. Abra um Pull Request

Veja [CONTRIBUTING.md](CONTRIBUTING.md) para mais detalhes.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja [LICENSE](LICENSE) para mais detalhes.

## ğŸ¯ Exemplo Completo

Confira `examples/basic_usage.py` para um exemplo completo de uso da biblioteca.